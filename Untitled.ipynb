{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f441c7a-8f5a-493e-b329-d1919536c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/23 21:27:03 WARN Utils: Your hostname, arduinum-pc resolves to a loopback address: 127.0.1.1; using 192.168.0.107 instead (on interface wlp2s0)\n",
      "24/07/23 21:27:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/23 21:27:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|ProductName| Category|\n",
      "+-----------+---------+\n",
      "|   ProductA|Category1|\n",
      "|   ProductB|Category2|\n",
      "|   ProductC|     NULL|\n",
      "|   ProductD|     NULL|\n",
      "|   ProductE|     NULL|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "def combine_products_and_categories(df_1, df_2):\n",
    "    \"\"\"\n",
    "    Комбинирует информацию о продуктах и их категориях, \n",
    "    включая продукты без категорий.\n",
    "    \"\"\"\n",
    "    \n",
    "    combined_df = df_1.union(df_2)\n",
    "    filtered_df = combined_df.select(\"ProductName\", \"Category\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # создаём сессию   \n",
    "    spark = SparkSession.builder.appName(\"Shop_app\").getOrCreate()\n",
    "\n",
    "    # DataFrame с продуктами и их категориями\n",
    "    data1 = [\n",
    "        (\"ProductA\", \"Category1\"), \n",
    "        (\"ProductB\", \"Category2\"), \n",
    "        (\"ProductC\", None)]\n",
    "    df1 = spark.createDataFrame(data=data1, schema=[\"ProductName\", \"Category\"])\n",
    "\n",
    "    # DataFrame с продуктами без категорий\n",
    "    data2 = [(\"ProductD\",), (\"ProductE\",)]\n",
    "    df2 = spark.createDataFrame(data=data2, schema=[\"ProductName\"])\n",
    "    \n",
    "    # # Добавляем столбец Category со значением None\n",
    "    df2 = df2.withColumn(\"Category\", lit(None))\n",
    "\n",
    "    combined_df = combine_products_and_categories( \n",
    "        df_1=df1, \n",
    "        df_2=df2\n",
    "    )\n",
    "    \n",
    "    combined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ba312-214d-44dc-8e54-35c9b0b67210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
